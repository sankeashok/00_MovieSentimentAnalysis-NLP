{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SL-ECT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1pXIYhBIU36PfLdeybwpcZ9WmXIHO33D3",
      "authorship_tag": "ABX9TyMkXB3lnj/8O31koZdsDcq8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankeashok/NLP/blob/master/SL_ECT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EW2rYgvMA-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fa8f05-e5bc-4bb7-f754-97ad2ea10b56"
      },
      "source": [
        "!wget https://www.dropbox.com/s/5bhhs3hpyiawhel/ect_02_data.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-25 06:48:44--  https://www.dropbox.com/s/5bhhs3hpyiawhel/ect_02_data.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/5bhhs3hpyiawhel/ect_02_data.csv [following]\n",
            "--2020-11-25 06:48:45--  https://www.dropbox.com/s/raw/5bhhs3hpyiawhel/ect_02_data.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc83bc21fd9dfde0657cc61451d3.dl.dropboxusercontent.com/cd/0/inline/BD0dchFVHg-8bNw47hMqYs8_O1axsccd259wjlLOFv5iQY_v_5mwieOuCcfjpbOpa2vp613tNBia8lTO_8wNOQ4egk1Xl1Lp3I3EPFfh4CnsTuIIoEuW809AZIiOvUlWQvc/file# [following]\n",
            "--2020-11-25 06:48:45--  https://uc83bc21fd9dfde0657cc61451d3.dl.dropboxusercontent.com/cd/0/inline/BD0dchFVHg-8bNw47hMqYs8_O1axsccd259wjlLOFv5iQY_v_5mwieOuCcfjpbOpa2vp613tNBia8lTO_8wNOQ4egk1Xl1Lp3I3EPFfh4CnsTuIIoEuW809AZIiOvUlWQvc/file\n",
            "Resolving uc83bc21fd9dfde0657cc61451d3.dl.dropboxusercontent.com (uc83bc21fd9dfde0657cc61451d3.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc83bc21fd9dfde0657cc61451d3.dl.dropboxusercontent.com (uc83bc21fd9dfde0657cc61451d3.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15560 (15K) [text/plain]\n",
            "Saving to: ‘ect_02_data.csv’\n",
            "\n",
            "ect_02_data.csv     100%[===================>]  15.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-25 06:48:46 (212 MB/s) - ‘ect_02_data.csv’ saved [15560/15560]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwVFpS7SrfIi"
      },
      "source": [
        "import os\n",
        "os.chdir = '/content/drive/MyDrive/AIML-NITW/6.SL/ECT2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyXXILDjr6_Q",
        "outputId": "7d186979-978c-44c7-c3aa-4ac5cbf4332c"
      },
      "source": [
        "!dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  ect_02_data.csv\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO-rSo3psCK5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout, Input, Flatten\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w-66vABDxbq"
      },
      "source": [
        "# Question 1.1\n",
        "    . Remove everything except number and character from text\n",
        "    . Tokenize data\n",
        "    . all text lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC3RlRDUDpcn"
      },
      "source": [
        "data = pd.read_csv('ect_02_data.csv').iloc[:, 0:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "GZGcCF_PDpgR",
        "outputId": "e6164d5b-584c-46bf-9f39-e0467308f9cc"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response_id</th>\n",
              "      <th>class</th>\n",
              "      <th>response_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>response_1</td>\n",
              "      <td>not_flagged</td>\n",
              "      <td>I try and avoid this sort of conflict</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>response_2</td>\n",
              "      <td>flagged</td>\n",
              "      <td>Had a friend open up to me about his mental ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>response_3</td>\n",
              "      <td>flagged</td>\n",
              "      <td>I saved a girl from suicide once. She was goin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>response_4</td>\n",
              "      <td>not_flagged</td>\n",
              "      <td>i cant think of one really...i think i may hav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>response_5</td>\n",
              "      <td>not_flagged</td>\n",
              "      <td>Only really one friend who doesn't fit into th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  response_id        class                                      response_text\n",
              "0  response_1  not_flagged              I try and avoid this sort of conflict\n",
              "1  response_2      flagged  Had a friend open up to me about his mental ad...\n",
              "2  response_3      flagged  I saved a girl from suicide once. She was goin...\n",
              "3  response_4  not_flagged  i cant think of one really...i think i may hav...\n",
              "4  response_5  not_flagged  Only really one friend who doesn't fit into th..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRKY3cRODpjZ"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9]+', ' ', text)\n",
        "    text = word_tokenize(text)\n",
        "    text = [word.lower() for word in text]\n",
        "    return ' '.join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQfQDJyIDpmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad475a6c-7f5e-4243-e784-7d544de85543"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "data['clean_text'] = data.response_text.apply(clean_text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lzIOwKUDp3m",
        "outputId": "f3eb9192-f222-4450-ad67-db8f0cc8daf3"
      },
      "source": [
        "data['clean_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                 i try and avoid this sort of conflict\n",
              "1     had a friend open up to me about his mental ad...\n",
              "2     i saved a girl from suicide once she was going...\n",
              "3     i cant think of one really i think i may have ...\n",
              "4     only really one friend who doesn t fit into th...\n",
              "                            ...                        \n",
              "75    now that i ve been through it although i m not...\n",
              "76    when my best friends mom past away from od ing...\n",
              "77    as a camp counselor i provide stability in kid...\n",
              "78    my now girlfriend used to have serious addicti...\n",
              "79    the one person i ever talked to it was because...\n",
              "Name: clean_text, Length: 80, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7XxfCFglFIg"
      },
      "source": [
        "# Question 1.2\n",
        "    . Tokenize using Keras tokenizer\n",
        "    . Convert text to sequence (texts_to_sequences)\n",
        "    . Pad the sequences to streamline (pad_sequences)\n",
        "    . Convert all to one_hot_encoding (OneHotEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZOQNtPgDqBV"
      },
      "source": [
        "def lstm_data(text, labels):\n",
        "    token = Tokenizer(10000)\n",
        "    token.fit_on_texts(text)\n",
        "    tokenized_data = token.texts_to_sequences(text)\n",
        "    tokenized_data = pad_sequences(tokenized_data, maxlen= 20, padding= \"post\")\n",
        "    encode_target = OneHotEncoder(sparse = False).fit(np.array(labels).reshape(-1,1))\n",
        "    labels_data = encode_target.transform(np.array(labels).reshape(-1,1))\n",
        "    return token, tokenized_data, labels_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXQ8Bb2zDqEA"
      },
      "source": [
        "tokenizer, text, labels = lstm_data(data.response_text.values, data['class'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBm9F82VDp1S"
      },
      "source": [
        "def transform_matrix(data, tokenizer):\n",
        "    output_shape_mat= [data.shape[0],\n",
        "                  data.shape[1],\n",
        "                  tokenizer.word_index.keys().__len__()] #Three dimensional matrix with samples, steps and number of uniques words as each dimension.\n",
        "    results_data= np.zeros(output_shape_mat) #creates new array with given dimensions.\n",
        "    \n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            results_data[i, j, data[i,j]-1]= 1 # In this for loop, we are looping over the shape of the training & test data and assigning the cell of above created zero matrix to 1. We are performing encoding on the unique words to obtain the transformation matrix\n",
        "    return results_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoq0yS2ODpx-"
      },
      "source": [
        "text = transform_matrix(text, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmu9hONtDpwD",
        "outputId": "a4c9ed72-b140-40ab-e1c4-e9cbfa8c34c4"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "       [[1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 1.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi68ueyIrSCg"
      },
      "source": [
        "# Question 1.3 - Model\n",
        "    . Embedding layer\n",
        "    . LSTM cells (128)\n",
        "    . Dense (2)\n",
        "    . Dropout (0.5)\n",
        "    . Dense (2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx4VcbjGrUNB",
        "outputId": "0c3c7e32-3686-4d4b-ca10-1c5189325557"
      },
      "source": [
        "text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 20, 677)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7c419pGrUQg"
      },
      "source": [
        "def lstm_model():\n",
        "    ip_layer = Input(shape=(text.shape[1],))\n",
        "    embedding_layer = Embedding(input_dim=tokenizer.word_index.keys().__len__()+1,output_dim=128,input_length=20)(ip_layer)\n",
        "    lstm_layer = LSTM(units= 128)(embedding_layer)\n",
        "    dense_layer_1= Dense(2, activation='relu')(lstm_layer)\n",
        "    drop_layer = Dropout(0.5)(dense_layer_1)\n",
        "    final_layer = Dense(2, activation='sigmoid')(drop_layer)\n",
        "    return Model(inputs= [ip_layer], outputs= [final_layer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNWnvwVPrUUH",
        "outputId": "bca36f44-6846-400b-ce4d-5c3b5c1470d5"
      },
      "source": [
        "lstm_model().summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 20)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 20, 128)           86784     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 258       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 6         \n",
            "=================================================================\n",
            "Total params: 218,632\n",
            "Trainable params: 218,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp5mgQpiyQ18"
      },
      "source": [
        "# Question 1.4 - Model Training\n",
        "    . Optimizer: Adam\n",
        "    . Epochs: 500\n",
        "    . Batch size: 30\n",
        "    . Save model as model.h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbeYxTQryTpM"
      },
      "source": [
        "model= lstm_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNzRj9eQyT3u"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THsqaGukyXOi"
      },
      "source": [
        "model.fit(text, labels, epochs=500, batch_size=30, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJPRQwZQyXSN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q3WFwvXyXXt"
      },
      "source": [
        "def lstm_data1(text, labels):\n",
        "    token = Tokenizer(filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
        "    token.fit_on_texts(text)\n",
        "    tokenized_data = token.texts_to_sequences(text)\n",
        "    tokenized_data = pad_sequences(tokenized_data, maxlen= 20, padding= \"post\")\n",
        "    encode_target = OneHotEncoder(sparse = False).fit(np.array(labels).reshape(-1,1))\n",
        "    labels_data = encode_target.transform(np.array(labels).reshape(-1,1))\n",
        "    return token, tokenized_data, labels_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SszXY7yuyXVf"
      },
      "source": [
        "tokenizer1, text1, labels1 = lstm_data1(data.response_text.values, data['class'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkC7mBdaDWaz"
      },
      "source": [
        "text1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrpgublcDWeS"
      },
      "source": [
        "def lstm_model1():\n",
        "    ip_layer = Input(shape=(text.shape[1],))\n",
        "    embedding_layer = Embedding(input_dim=tokenizer.word_index.keys().__len__()+1,output_dim=128,input_length=20)(ip_layer)\n",
        "    lstm_layer = Bidirectional(LSTM(128))(embedding_layer)\n",
        "    dense_layer_1= Dense(64, activation='relu')(lstm_layer)\n",
        "    drop_layer = Dropout(0.5)(dense_layer_1)\n",
        "    final_layer = Dense(2, activation='sigmoid')(drop_layer)\n",
        "    return Model(inputs= [ip_layer], outputs= [final_layer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ4bkpq8DWhx"
      },
      "source": [
        "lstm_model1().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xe-6Q1BF_9M"
      },
      "source": [
        "model1= lstm_model1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpblC2RMGADO"
      },
      "source": [
        "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfmBUETQGAI7"
      },
      "source": [
        "model1.fit(text, labels, epochs=800, batch_size=40, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8YVxeCYF-bC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}